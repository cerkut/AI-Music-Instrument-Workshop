<!-- Slide number: 1 -->

# Music and AI: Embedded (and Embodied)

Cumhur Erkut, cer@create.aau.dk

Associate Professor, Dr. Sc. (Tech)

https://cerkut.github.io

![1737073391385](image/MusicAIEmbedded/1737073391385.png)

### Based on August’23 RMC workshop

![1737073480926](image/MusicAIEmbedded/1737073480926.png)

## Principles of AI Disruption 

2.1 A new technology is accepted in production in one of the following ways: a) `<span style="color:pink">`replacement, b) `<span style="color:red">`insertion, and c) `<span style="color:blue">`disruption (point of no return).

2.2 AI-assisted (music) production currently proposes value by a) and b). It is not a disruptive yet.

2.3 A disruptive technology is accepted if and only if (iff) when its value proposition is significantly higher than the risks.

## Example: Birth of Modern Music Production

![1737074297517](image/MusicAIEmbedded/1737074297517.png)"A disruption" by Cumhur Erkut is licensed under CC BY-SA

### Notes:

In the case of Kind of Blue there were two producers: Teo Macero and Irving Townsend”, said jazz historian Eric Nisenson.

<!-- Slide number: 4 -->

## But the disruption is near

Besides sound and music, the living **individual body or social bodies** are the final frontiers for big data and deep learning!

According to Zuboff, economies of scale, scope, and action are forms of harm. 

* **Economies of scale**: cost advantages when producing in large quantities.
* **Economies of scope**: cost advantages when producing variety 
* **Economies of action**: cost advantages when companies use their market power to influence the behavior of other companies.

![1737074602580](image/MusicAIEmbedded/1737074602580.png)

### Notes:

<!-- Slide number: 5 -->

![Graphical user interface, application Description automatically generated](Picture8.jpg)

![A logo of a company

Description automatically generated](Picture3.jpg)

![Picture 5](Picture5.jpg)

![A book cover of a book

Description automatically generated](Picture7.jpg)

![A flag with a logo on it

Description automatically generated](Picture8.jpg)

![Picture 9](Picture9.jpg)

### Notes:

See also https://github.com/AI-Guru/music-generation-research (stopped in 2023)!

<!-- Slide number: 6 -->

# VR / Tangible HCI Rehabilitation

### TODO Notes: bullets and items from this point onwards

Example 1:

<!-- Slide number: 7 -->

![Picture 1](Picture1.jpg)

![A logo of a company

Description automatically generated](Picture2.jpg)
https://audiobox.metademolab.com

![Picture 4](Picture4.jpg)

<!-- Slide number: 8 -->

![Picture 5](Picture5.jpg)

Snapshot of SIGGRAPH 97 demo

DIVA is a collaborative research group of the following topics:
* Real-time automatic character animation
* Interaction through motion analysis, especially conductor following.
* Sound generation with physical instrument models
* Acoustics modelling and auralization
* EVE - The Experimental Virtual Environment

![Picture 2](Picture2.jpg)
DIVA - Digital Interactive Virtual Acoustics (‘98 Arrival to Finland, ’13 to Denmark )

![Image](Image.jpg)

### Notes:

<!-- Slide number: 9 -->

# Neural Nets from scratch (Python was not invented :-)

### Notes:

<!-- Slide number: 10 -->

![Image](Image.jpg)

![Image](Image.jpg)

![Image](Image.jpg)

![Picture 2](Picture2.jpg)
ANNs as optimizers

### Notes:

Also a clear timeframe table would be nice?

<!-- Slide number: 11 -->

https://web.archive.org/web/20170305231733/http://silakka.fi/compositions/pcm-035553/

![A group of people sitting around a table with laptops

Description automatically generated](Picture5.jpg)

### Notes:

<!-- Slide number: 12 -->

https://web.archive.org/web/20170305231733/http://silakka.fi/compositions/pcm-035553/

![A group of people sitting around a table with laptops

Description automatically generated](Picture5.jpg)

### Notes:

<!-- Slide number: 13 -->

https://web.archive.org/web/20170305231733/http://silakka.fi/compositions/pcm-035553/

![A group of people sitting at tables with computers

Description automatically generated](Picture2.jpg)

![Google Shape;76;g94c2f5be6a_0_11](GoogleShape76g94c2f5be6a_0_11.jpg)

![A diagram of a human

Description automatically generated](GoogleShape82ga4abce899c_0_1.jpg)

![A diagram of a discrepancy

Description automatically generated](Picture3.jpg)

![Image](Image.jpg)

### Notes:

<!-- Slide number: 14 -->

![Picture 5](Picture5.jpg)

Snapshot of SIGGRAPH 97 demo

DIVA is a collaborative research group of the following topics:
Real-time automatic character animation
Interaction through motion analysis, especially conductor following.
Sound generation with physical instrument models
Acoustics modelling and auralization
EVE - The Experimental Virtual Environment

![Picture 2](Picture2.jpg)
DIVA - Digital Interactive Virtual Acoustics (‘98 Arrival to Finland, ’13 to Denmark )

![Image](Image.jpg)

### Notes:

<!-- Slide number: 15 -->

# Multisensory Experience Lab (CPH, DK)

![Picture 2](Picture2.jpg)

![Picture 3](Picture3.jpg)

![Picture 2](Picture2.jpg)

<!-- Slide number: 16 -->

![Picture 4](Picture4.jpg)
A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.

![Image](Image.jpg)

### Notes:

Machine Learning is the training of a model from data that generalizes a decision against a performance measure.

<!-- Slide number: 17 -->

![Image](Image.jpg)

![Picture 2](Picture2.jpg)

![Geometric foundations of Deep Learning | by Michael Bronstein | Towards  Data Science](Picture2.jpg)

### Notes:

Machine Learning is the training of a model from data that generalizes a decision against a performance measure.

<!-- Slide number: 18 -->

] Engel, J., Hantrakul, L., Gu, C., & Roberts, ADDSP: Differentiable Digital Signal Processing, In, Proc. ICLR (2020)

# 2019

![Image](Image.jpg)

### Notes:

The differentiability allows us to use domain specific-expertise in sound, music, and (even possibly in the future movement computing), since the dawn of digital computers. Running an encoder / decoder at the front end, and it calculates f0 and general loudness, and further breaks down the decoded signal to harmonic & filtered noise components, plus a convolution-based reverberator. All the DSP is done feedforward filters in the frequency domain. The differentiability allows us to use domain specific-expertise in sound, music, and (even possibly in the future movement computing), since the dawn of digital computers. Running an encoder / decoder at the front end, and it calculates f0 and general loudness, and further breaks down the decoded signal to harmonic & filtered noise components, plus a convolution-based reverberator. All the DSP is done feedforward filters in the frequency domain.

<!-- Slide number: 19 -->

# Deployment: Audio Plugins

![Diagram

Description automatically generated](Picture2.jpg)

### Notes:

This is our deployment target.

<!-- Slide number: 20 -->

# Example: VST, AU, and other plugins

![Graphical user interface, application

Description automatically generated](Picture8.jpg)

<!-- Slide number: 21 -->

![droppedImage.png](droppedImagepng.jpg)
https://lh3.googleusercontent.com/wkmQM4waAjsFw056qS0VCj5UcvooLLUI5_EwhaCrx6i69M5qu6BicY6TlhXcd6tYDd6_JHE3u7av9ImohJ2XIJy2tIVm3nqChBuA=w2048-rw-v1

![Image](Image.jpg)

![Image](Image.jpg)

![Image](Image.jpg)
The BIG Dream:Differentiable Neural Audio Computing as a Foundation Model? (Graves et al, 2016)

### Notes:

Return to mother ship of DeepMIND!
I usually tell my students that DSP is all about memory access, and simple arithmetic, but possibly with complex numbers  What if we embed E2E learning into domain-specific programming?

<!-- Slide number: 22 -->

# Example 1: Resurrecting the Tromba Marina

### Notes:

<!-- Slide number: 23 -->

# Example 2: Generative Choreographies

### Notes:

Kaspersen, Esbern, Dawid Górny, Cumhur Erkut, and George Palamas. 2020. “Generative Choreographies: the Performance Dramaturgy of the Machine.” In, nil. doi:10.5220/0008990403190326.
https://github.com/dawidgorny/dance2dance
https://dawidgorny.com/

<!-- Slide number: 24 -->

![Picture 5](Picture5.jpg)

![Picture 6](Picture6.jpg)

### Notes:

The differentiability allows us to use domain specific-expertise in sound, music, and (even possibly in the future movement computing), since the dawn of digital computers. Running an encoder / decoder at the front end, and it calculates f0 and general loudness, and further breaks down the decoded signal to harmonic & filtered noise components, plus a convolution-based reverberator. All the DSP is done feedforward filters in the frequency domain. The differentiability allows us to use domain specific-expertise in sound, music, and (even possibly in the future movement computing), since the dawn of digital computers. Running an encoder / decoder at the front end, and it calculates f0 and general loudness, and further breaks down the decoded signal to harmonic & filtered noise components, plus a convolution-based reverberator. All the DSP is done feedforward filters in the frequency domain.

<!-- Slide number: 25 -->

# Impact at Aalborg University

Boris Kuznetsov: Differentiable IIR Filters For Machine Learning Applications, MSc Thesis, 2020 (with J. Parker)
Christie Laurent: Model-based Analysis and Synthesis of Aging Effects on Human Voice Production, MSc Thesis, 2020
Ganis, Knudsen, Lyster, Otterbein, Südholt, and Erkut. 2021. “Real-Time Timbre Transfer and Sound Synthesis using DDSP.” In Proc. Sound and Music Computing Conf., https://doi.org/10.5281/zenodo.5043235
Alonso, Juan, and Cumhur Erkut. 2021. “Explorations of Singing Voice Synthesis Using DDSP.”  In Proc. Sound and Music Computing Conf., https://doi.org/10.5281/zenodo.5043851.
Juan Alonso: Differentiable FM synthesis: Applications to Timbre transfer and Latent space, MSc Thesis, 2021: https://juanalonso.github.io/ddsp_fm/
2020-2021

<!-- Slide number: 26 -->

# Example: Parametric Latent Space in Differentiable FM Sytnhesis (Alonso’21)

![Picture 7](Picture7.jpg)

<!-- Slide number: 27 -->

# Impact at Aalborg University

Parametric Tuning of Extended Reverberation Algorithm Using Neural Networks, Søren Vøgg Krabbe Lyster
Vocoding with Differentiable Digital Signal Processing: Development of a Real-Time Vocal Effect Plugin, David Südholt
Design, Development and Evaluation of Differentiable Max Objects for Real-time Neural Sound Synthesis and Timbre Transfer using Differentiable Digital Signal Processing, Robin Otterbein
Pruning Deep Neural Network Models of Guitar Distortion Effects”, David Südholt, Alec Wright, Cumhur Erkut, and Vesa Välimäki, IEEE Transactions on Audio, Speech and Language Processing, December 2022
ImprovAIze: Using deep learning to improve real-time motion recognition and affective experiences for human-machine interaction (Jochum, Erkut, Dahl, Overholt, Palamas, Bai, and Otterbein)

2022

<!-- Slide number: 28 -->

# Three pillars of ML lifecycle: Model, Data, and Code

![Picture 4](Picture4.jpg)

### Notes:

This is our definition in teacthing.

<!-- Slide number: 29 -->

![Diagram

Description automatically generated](Picture7.jpg)

### Notes:

<!-- Slide number: 30 -->

# Three pillars of ML lifecycle: Model, Data, and Code

![Picture 2](Picture2.jpg)

<!-- Slide number: 31 -->

# Three pillars of ML lifecycle: Model, Data, and Code

![Picture 2](Picture2.jpg)

![Picture 2](Picture2.jpg)

![Graphical user interface, application

Description automatically generated](Picture8.jpg)

![Picture 10](Picture10.jpg)

![Picture 13](Picture13.jpg)
Communities

### Notes:

<!-- Slide number: 32 -->

# Three pillars of ML lifecycle: Model, Data, and Code

![Picture 2](Picture2.jpg)

![Picture 2](Picture2.jpg)

### Notes:

in an article published in 2020 by Algorithmia, unreasonably long model deployment timelines were unveiled, in which only 14% of the surveyed companies can manage the deployment of a new model to production in under 7 days, 35% of the surveyed companies would need at least 3 months. There are still companies that require years. Adopting model deployment principles early on can eliminate this showstopper entirely. So what are the model deployment principles? We focus on the tools, but principles?

<!-- Slide number: 33 -->

![Graphical user interface, application

Description automatically generated](Picture8.jpg)
https://github.com/archinetai/audio-ai-timeline

### Notes:

<!-- Slide number: 34 -->

![Picture 1](Picture1.jpg)
https://mattturck.com/mad2024

### Notes:

https://mattturck.com/mad2024

<!-- Slide number: 35 -->

![A picture containing text, indoor, desk

Description automatically generated](Picture4.jpg)
NSynth+
Sensel Morph
NVIDIA Jetson Nano
Moog Studio, with Moog and Generative Deep Learning Playing Cards
NVIDIA
Jetson
Orin
SpaceMouse
Uno Hybrid Synth
ZED 2 Depth
Genelec Speakers
Studio One with Setup Memory
3D Lazer Projector
HTC Vive Pro

<!-- Slide number: 36 -->

![Picture 1](Picture1.jpg)

![Picture 2](Picture2.jpg)

<!-- Slide number: 37 -->

![Picture 1](Picture1.jpg)

<!-- Slide number: 38 -->

# Our MLOps: Bridging AI and X-verse with ∂-models

![Picture 5](Picture5.jpg)

![Diagram

Description automatically generated](Picture2.jpg)

![Image](Image.jpg)

![Graphical user interface, application

Description automatically generated](Picture8.jpg)

### Notes:

Differentiable programming not elaborated until here. Check the process at FB with Kotlin?

<!-- Slide number: 39 -->

# Our MLOps? Karpathy (SW 2.0) +  HCI (IxD) + Academia

![Picture 5](Picture5.jpg)
Lones, Michael A. 2021. “How to Avoid Machine Learning Pitfalls: A Guide for Academic Researchers.” arXiv. https://doi.org/10.48550/arxiv.2108.02497

Scott H. Hawley @drscotthawley
Here are slides from my talk @acousticsorg: "Development tools for deep learning models of acoustical signal processing" https://hedges.belmont.edu/ml-audio-dev-tools.pdf…

![Picture 2](Picture2.jpg)

![Machine Learning is Going Real-Time | Tecton](Picture8.jpg)
Kreuzberger, Dominik, Niklas Kühl, and Sebastian Hirschl. 2022. “Machine Learning Operations (MLOps): Overview, Definition, and Architecture.” CoRR.
Tamburri, Damian A. 2020. “Sustainable MLOps: Trends and Challenges.” 2020 22nd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC) 00: 17–23. doi:10.1109/synasc51798.2020.00015.

![Picture 4](Picture4.jpg)

<!-- Slide number: 40 -->

![Picture 1](Picture1.jpg)
NOW
Future
Scott H. Hawley @drscotthawley
https://hedges.belmont.edu/ml-audio-dev-tools.pdf

![Picture 2](Picture2.jpg)

<!-- Slide number: 41 -->

![Picture 5](Picture5.jpg)

![Picture 6](Picture6.jpg)

![Picture 7](Picture7.jpg)

### Notes:

https://github.com/abargum/pitch-enc

<!-- Slide number: 42 -->

# Embedded and embodied: DJs and Virtuoses of AI

![Picture 4](Picture4.jpg)

![Geometric foundations of Deep Learning | by Michael Bronstein | Towards  Data Science](Picture2.jpg)

![Image](Image.jpg)

![A red and white sign

Description automatically generated with low confidence](Picture3.jpg)
This Photo by Unknown Author is licensed under CC BY-SA
https://paperswithcode.com/area/audio

![Picture 11](Picture11.jpg)
Şimşekli, Umut, Antti Jylhä, Cumhur Erkut, and A T Cemgil. “Real-Time Recognition of Percussive Sounds by a Model-Based Method.” EURASIP Journal on Advances in Signal Processing 2011 (1): 291860.

![Picture 13](Picture13.jpg)

![Icon

Description automatically generated](Picture15.jpg)

![Chart, scatter chart

Description automatically generated](Picture18.jpg)

![Image](Image.jpg)

![Image](Image.jpg)

![Picture 21](Picture21.jpg)

![Picture 2](Picture2.jpg)
